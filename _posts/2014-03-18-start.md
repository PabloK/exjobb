---
layout: post
title:  "Start"
date:   2014-03-18
categories:
---

# Image Quality Analysis (IQA)

IQA refers to measuring the quality of an image. It can be performed through subjective or objective assessment. Often the goal is to identify the quality of an image after some type of distortion has been applied. IQA is widely used to evaluate how good a image compression algorithm is with respect to the quality of the compressed image compared to the original.

*(Self note: transmitter, channel, receiver.)*


## Subjective Assessment

A human is involved when evaluating an image, then the combined results are evaluated statistically. This isn't a scalable solution.


## Objective Assessment

Objective assessment methods can be divided into three approaches: *full-reference*, *no-reference*, and *reduced-reference*. "[A] reference image [is] assumed to be error free and of highest quality. In reduced reference methods, features are extracted from the reference image and the method only compares this limited set of features with the image the quality of which has to be evaluated, which we will call *subject* image."

Read more in *Zhou Wang and Alan C. Bovik, Modern Image Quality Assessment. Morgan & Claypool Publishers, 2006*.

### Full-Reference (FR) IQA

* Mean Square Error (MSE)
* Peak Signal to Noise Ratio (PSNR)
* Structural Similarity Index Map (SSIM)

MSE and PSNR don't take into account the fidelity of a signal (i.e. what the image looks like), while SSIM works much better at respecting what makes humans regard a signal as high or low quality. SSIM won't work very well when there isn't a reference image, as in the case of procedurally rendered images. However, whether I need to take such situations into account isn't clear.

[Here][perceptual-digital-imaging] it's argued that the current state of full-reference IQA is very stable.


### Reduced-Reference

* Scale-Invariant Feature Transform (SIFT)

*See: Lowe, David G., Object recognition from local scale-invariant features, Proceedings of the International Conference on Computer Vision. 2. pp. 1150-1157. 1999.*


### No-Reference (NR) IQA

NR IQA algorithms are broadly classified as distortion-specific, and those who are distortion-agnostic or holistic. In other words, they either assume that an image has been affected by a certain type of distortion – most commonly a compression algorithm – or they don't assume anything. There are only a few holistic NR IQA algorithms.

*See: Robert Herzog, Martin Cadik, Tunc O. Aydin, Kwang In Kim, Karol Myszkowski and Hans-P. Seidel, NoRM: No-Reference Image Quality Metric for Realistic Image Synthesis, The Eurographics Association and Blackwell Publishing Ltd., 2012.*


## Neural Networks

[Amann et. al][amann et. al] suggests using Self Organizing Maps (SOM). This produces a 2D map where similar images cluster together. Without training the computer to recognize correct or incorrect images, this method will merely group them together. SOM is also known as a Kohonen map. See: *Kohonen, Teuvo. Self-Organized Formation of Topologically Correct Feature Maps. Biological Cybernetics 43 (1): 59-69. 1982.*

It is also suggested that other types of neural networks could be used to classify images. The whole process seems to revolve around the following steps (I'm not completely sure that this is entirely correct).

1. Create a *feature-vector*. Basically it's an n-dimensional vector that stores a bunch of different features of the image, like color distribution, lightness, low-, mid-, and high-frequency gradient, edginess, and any other quantifiable thing.
2. Select which of the features are best suited to distinguish between a correct and an incorrect image. This could also be done with the help of [principal component analysis][pca].
3. Train the computer with a batch of images, manually classifying the images as *correct*, *error-1*, *error-2*, etc.
4. Let the computer decide what class an new image that wasn't in the training set belongs to, e.g. *correct*, *error-1*, *error-2*, etc.


# Challenges with multiple devices

## Collection of data

We want to test the output on multiple different devices, which encompasses multiple screen-sizes. So images will not necessarily be the same size. They *could* be by demanding the cnavas to be of a specific size; the smallest screen size would then dictate that size. It's not very likely, but a smaller screen size may need to be added at a later stage. Smaller screen size also reduces the amount of data that can be used.

It's possible that upsampling or downsampling might be good enough for practical purposes.

What about adding a "magnifying glass"? Take a screenshot of a certain region with a specific size. Sweep over the scene similarly to how a CMOS-camera reads the sensor line-by-line.


## What types of errors should we expect to see?

Which method that is best suited for us relies heavily on the types of errors (distortion) we expect to see.

The range of devices that run WebGL is pretty wide... computers, tablets, mobile phones, television sets, set-top boxes. So, we can't make too many assumptions regarding the hardware used to run a WebGL program. What varies? Memory size, processing power, more more more. What qualities do these errors most likely exhibit?


## Resources

[Bovik, Alan Conrad][bovik] looks like he knows his stuff.

[Using Image Quality Assessment to Test Rendering Algorithms][amann et. al], Amann et. al

[OpenCV][opencv] - Open Source Computer Vision

[Perceptual Digital Imaging: Methods and Applications][perceptual-digital-imaging]


[amann et. al]: http://wscg.zcu.cz/wscg2013/program/full/E43-full.pdf
[opencv]: http://opencv.org
[pca]: http://en.wikipedia.org/wiki/Principal_component_analysis
[bovik]: http://scholar.google.se/citations?user=p-PC50wAAAAJ&hl=sv&oi=ao
[perceptual-digital-imaging]: http://books.google.se/books?id=ciOF1H-wZacC&pg=PA81#v=onepage&q&f=false